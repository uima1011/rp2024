# konfiguriere Trainings- und Umgebungsparameter hier:
train:
  name: "DQN_normalized_small_observationspace_corrected_padding" # training name
  dir: "/home/group1/workspace/data"                              # directory to save the training data
  timesteps: 10000                                                # number of timesteps
  resetTimesteps: False                                           # reset after every episode
  policy: "MlpPolicy"                                             # 2 hidden layers of 64 neurons each
  DQN:
    gamma: 0.99                                                   # discount factor
    learning_rate: 1e-4                                           # learning rate for more stable learning
    buffer_size: 100000                                           # size of the replay buffer
    batch_size: 64                                                # batch size (default value for DQN)
    train_freq: 4                                                 # training after every 4th action
    target_update_interval: 1000                                  # update the target network after 1000 steps
    exploration_fraction: 0.1                                     # 10% of training time for exploration
    exploration_final_eps: 0.02                                   # minimum exploration value
    verbose: 1                                                    # output information (1=verbose)
  env:
    actions: 4                                                    # number of possible actions
    render: True                                                  # enable visualization
    assetsPath: "/home/group1/workspace/assets"                   # directory for the assets
    maxSteps: 200                                                 # maximum number of steps per episode
    colours: ["red", "green"]                                     # colors of the objects
    objects:
      number: 3                                                   # maximum number of objects
      dirs: ["signs", "cubes"]                                    # directories of the objects
      parts: ["plus", "cube"]                                     # parts of the objects
      states: 3                                                   # number of states (x,y,z)
    goals:
      states: 3                                                   # number of states (x,y,z)
    robot:
      states: 2                                                   # number of states (x,y)
